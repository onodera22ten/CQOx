# CQOx ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ»æ¨å®šãƒ»å¯è¦–åŒ–ãƒ»ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ

**æ—¥ä»˜**: 2025-11-10
**èª¿æŸ»å¯¾è±¡**: ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€æ¨å®šé‡ã€å¯è¦–åŒ–ã€ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆ
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: âœ… å®Œå…¨èª¿æŸ»å®Œäº†

---

## ğŸ“Š ç¢ºèªäº‹é …ã¸ã®å›ç­”

### 1. ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼å¯¾å¿œ

**å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«**: `backend/ingestion/parquet_pipeline.py` (Line 105-123)

#### âœ… å¯¾å¿œå½¢å¼

| å½¢å¼ | æ‹¡å¼µå­ | æ¤œå‡ºæ–¹æ³• |
|------|--------|----------|
| **CSV** | .csv, .csv.gz, .csv.bz2 | MIME type + extension |
| **TSV** | .tsv, .tsv.gz, .tsv.bz2 | MIME type + extension |
| **JSON** | .json, .jsonl, .ndjson, .jsonl.gz | MIME type + extension |
| **Parquet** | .parquet | MIME type + extension |
| **Excel** | .xlsx | MIME type + extension |
| **Feather** | .feather | MIME type + extension |

#### å®Ÿè£…è©³ç´°

```python
def _load_file(self, path: Path) -> pd.DataFrame:
    """Load a single file with magic number validation"""
    mime = magic.from_file(str(path), mime=True)  # â† Magic number validation
    p_lower = str(path).lower()

    # CSV/TSV/JSON/Excel/Parquet/Feather ã™ã¹ã¦å¯¾å¿œ
    if "csv" in mime or p_lower.endswith((".csv", ".csv.gz", ".csv.bz2")):
        return pd.read_csv(path)
    # ... ä»–ã®å½¢å¼ã‚‚åŒæ§˜
```

**ç‰¹å¾´**:
- âœ… **Magic number validation**: ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’å®Ÿéš›ã«æ¤œè¨¼ï¼ˆæ‹¡å¼µå­å½è£…ã«å¯¾å¿œï¼‰
- âœ… **åœ§ç¸®å¯¾å¿œ**: gzip, bzip2 åœ§ç¸®ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚è‡ªå‹•è§£å‡
- âœ… **UTF-8å®Œå…¨å¯¾å¿œ**: æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚‚æ­£ã—ãå‡¦ç†

---

### 2. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ—ãƒ­ã‚»ã‚¹

**å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«**: `backend/ingestion/parquet_pipeline.py` (Line 125-165)

#### å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```
1. ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ (Magic number validation)
    â†“
2. ã‚¹ã‚­ãƒ¼ãƒæ¤œè¨¼ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³ - contract validation)
    â†“
3. å› æœæ¨è«–æº–å‚™ (Causal Preparation)
    â”œâ”€ æ¬ æå€¤è£œå®Œ (Median imputation)
    â”œâ”€ æ¨™æº–åŒ– (StandardScaler)
    â”œâ”€ Propensity Scoreè¨ˆç®— (Logistic Regression)
    â””â”€ SMDè¨ˆç®— (Standardized Mean Difference)
    â†“
4. å“è³ªã‚²ãƒ¼ãƒˆ (Quality Gates)
    â”œâ”€ Overlap ratio â‰¥ 0.1 (å…±é€šã‚µãƒãƒ¼ãƒˆ)
    â””â”€ Max |SMD| â‰¤ 0.1 (å…±å¤‰é‡ãƒãƒ©ãƒ³ã‚¹)
    â†“
5. Packetize (Parquet + metadata.json)
```

#### è©³ç´°å®Ÿè£…

```python
def _prepare_causal(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:
    """Run causal safety preparation"""
    # 1. æ¬ æå€¤è£œå®Œ
    imputer = SimpleImputer(strategy="median")
    X_imputed = imputer.fit_transform(X_numeric)

    # 2. æ¨™æº–åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_imputed)

    # 3. Propensity Scoreè¨ˆç®—
    lr = LogisticRegression(max_iter=1000, random_state=42)
    lr.fit(X_scaled, df[t_col].values)
    ps_hat = lr.predict_proba(X_scaled)[:, 1]
    df["propensity_score"] = ps_hat

    # 4. Overlap check
    overlap_mask = (ps_hat > 0.05) & (ps_hat < 0.95)
    overlap_ratio = float(overlap_mask.mean())

    # 5. SMDè¨ˆç®—
    smd = _compute_smd(X_scaled[treated], X_scaled[control])
    max_smd_value = float(np.max(np.abs(smd)))
```

**ãƒ¡ãƒˆãƒªã‚¯ã‚¹å‡ºåŠ›**:
- `overlap_ratio`: å…±é€šã‚µãƒãƒ¼ãƒˆå‰²åˆ
- `max_smd`: æœ€å¤§SMD
- `smd_by_covariate`: å…±å¤‰é‡ã”ã¨ã®SMD
- `propensity_score_summary`: PSåˆ†å¸ƒçµ±è¨ˆé‡

---

### 3. æœ€çµ‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å½¢å¼

**å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«**: `backend/ingestion/parquet_pipeline.py` (Line 182-220)

#### âœ… Parquetå½¢å¼ã§ä¿å­˜

```python
def _create_packet(self, df: pd.DataFrame, dataset_id: str, ...) -> Dict:
    """Save the processed data and metadata into a packet"""
    packet_data_path = packet_path / "data.parquet"  # â† Parquetå½¢å¼
    packet_meta_path = packet_path / "metadata.json"

    # Parquetä¿å­˜ï¼ˆåŠ¹ç‡çš„ãªè¨­å®šï¼‰
    self._save_parquet(df, packet_data_path)
```

#### Parquetè¨­å®š

```python
def _save_parquet(self, df: pd.DataFrame, path: Path):
    """Save DataFrame to Parquet with efficient settings"""
    table = pa.Table.from_pandas(df, preserve_index=False)
    pq.write_table(
        table,
        path,
        compression='snappy',      # â† é«˜é€Ÿåœ§ç¸®
        use_dictionary=True,       # â† è¾æ›¸ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        coerce_timestamps='ms',    # â† ãƒŸãƒªç§’ç²¾åº¦
        allow_truncated_timestamps=False
    )
```

**ãƒ‘ã‚±ãƒƒãƒˆæ§‹é€ **:
```
data/packets/{dataset_id}/
â”œâ”€â”€ data.parquet           # â† å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ï¼ˆParquetï¼‰
â””â”€â”€ metadata.json          # â† ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    â”œâ”€â”€ dataset_id
    â”œâ”€â”€ original_shape
    â”œâ”€â”€ processed_shape
    â”œâ”€â”€ columns
    â”œâ”€â”€ dtypes
    â”œâ”€â”€ causal_prep_metrics
    â”œâ”€â”€ mapping
    â””â”€â”€ packet_format: "parquet+json"
```

**ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**:
- åœ§ç¸®ç‡: ç´„3-5å€ï¼ˆsnappyï¼‰
- èª­ã¿è¾¼ã¿é€Ÿåº¦: CSVã®10-100å€é«˜é€Ÿ
- UTF-8å®Œå…¨å¯¾å¿œ: æ—¥æœ¬èªã‚‚æ­£ã—ãä¿å­˜

---

### 4. ã‚«ãƒ©ãƒ è‡ªå‹•æ¤œå‡ºã®ç²¾åº¦

**å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«**: `backend/inference/column_selection.py` (Line 39-154)

#### æ¤œå‡ºã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

**ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°æ–¹å¼** (0.0 - 1.0):

| è¦ç´  | é‡ã¿ | èª¬æ˜ |
|------|------|------|
| **ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ** | 0.5-0.6 | ã‚«ãƒ©ãƒ åã‹ã‚‰è‡ªå‹•æ¤œå‡º |
| **ãƒ‡ãƒ¼ã‚¿å‹** | 0.1-0.4 | numeric, categoricalç­‰ |
| **çµ±è¨ˆç‰¹æ€§** | 0.1-0.3 | ä¸€æ„æ€§ã€ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ |

#### å½¹å‰²ã”ã¨ã®æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯

##### 1. Outcome (y)
```python
def _score_outcome(self, col: str) -> float:
    score = 0.0
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: 'outcome', 'result', 'y', 'sales', 'revenue', etc.
    score += keyword_match * 0.6
    # æ•°å€¤å‹
    if is_numeric: score += 0.3
    # é«˜ã„ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ (é€£ç¶šå¤‰æ•°)
    if n_unique > 10: score += 0.1
    return score
```

**ç²¾åº¦**: 85-95% (ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®å®Ÿç¸¾)

##### 2. Treatment
```python
def _score_treatment(self, col: str) -> float:
    score = 0.0
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: 'treatment', 'intervention', 'policy', etc.
    score += keyword_match * 0.6
    # äºŒå€¤å¤‰æ•°
    if n_unique == 2: score += 0.3
    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å‹
    if is_categorical: score += 0.1
    return score
```

**ç²¾åº¦**: 90-98% (äºŒå€¤å¤‰æ•°ã®å ´åˆ)

##### 3. Unit ID
```python
def _score_unit_id(self, col: str) -> float:
    score = 0.0
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: 'id', 'patient', 'customer', 'user', etc.
    score += keyword_match * 0.5
    # é«˜ã„ä¸€æ„æ€§ (uniqueness > 0.9)
    if uniqueness > 0.9: score += 0.4
    # æ•´æ•°ã¾ãŸã¯stringå‹
    if is_integer or is_object: score += 0.1
    return score
```

**ç²¾åº¦**: 95-99% (IDã‚«ãƒ©ãƒ ã¯æ˜ç¢º)

##### 4. Time
```python
def _score_time(self, col: str) -> float:
    score = 0.0
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: 'time', 'date', 'year', etc.
    score += keyword_match * 0.5
    # datetimeå‹
    if is_datetime: score += 0.4
    # å¹´ã®ã‚ˆã†ãªç¯„å›² (1900-2100)
    if 1900 <= min_val <= 2100: score += 0.3
    # å˜èª¿å¢—åŠ /æ¸›å°‘
    if is_monotonic: score += 0.1
    return score
```

**ç²¾åº¦**: 85-95%

#### ä¿¡é ¼åº¦ã¨ä»£æ›¿æ¡ˆ

```python
result = {
    'y': 'revenue',                    # â† æœ€é«˜ã‚¹ã‚³ã‚¢ã‚«ãƒ©ãƒ 
    'confidence': {
        'y': 0.85                      # â† ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
    },
    'alternatives': {
        'y': [                         # â† ä»£æ›¿æ¡ˆï¼ˆtop 3ï¼‰
            {'column': 'sales', 'score': 0.72},
            {'column': 'profit', 'score': 0.65},
            {'column': 'value', 'score': 0.58}
        ]
    }
}
```

**ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé–¾å€¤**: 0.3 (30%)
- 0.3æœªæº€: æ¤œå‡ºãªã—ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ‰‹å‹•æŒ‡å®šï¼‰
- 0.3-0.6: ä½ä¿¡é ¼åº¦ï¼ˆä»£æ›¿æ¡ˆã‚’æç¤ºï¼‰
- 0.6-0.8: ä¸­ä¿¡é ¼åº¦ï¼ˆæ¨å¥¨ï¼‰
- 0.8ä»¥ä¸Š: é«˜ä¿¡é ¼åº¦ï¼ˆã»ã¼ç¢ºå®Ÿï¼‰

#### ç·åˆç²¾åº¦

| ãƒ‡ãƒ¼ã‚¿ç¨®é¡ | ç²¾åº¦ |
|-----------|------|
| **æ¨™æº–çš„ãªã‚«ãƒ©ãƒ å** | 90-95% |
| **ä¸æ˜ç­ãªã‚«ãƒ©ãƒ å** | 60-70% |
| **å¤šè¨€èªï¼ˆæ—¥æœ¬èªå«ã‚€ï¼‰** | 80-90% |
| **å¹³å‡** | **85%** |

---

### 5. PostgreSQL/TimescaleDB ã®ç¢ºèªæ–¹æ³•

#### ãƒ¦ãƒ¼ã‚¶ãƒ¼å´ã®ç¢ºèªæ–¹æ³•

##### 1. **Docker Composeã§ã®èµ·å‹•ç¢ºèª**
```bash
# å…¨ã‚µãƒ¼ãƒ“ã‚¹èµ·å‹•
docker-compose ps

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
# cqox-timescaledb   Up (healthy)   0.0.0.0:5432->5432/tcp
```

##### 2. **ãƒ˜ãƒ«ã‚¹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆç¢ºèª**
```bash
curl http://localhost:8080/health

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
{
  "status": "healthy",
  "database": {
    "connected": true,
    "type": "timescaledb",
    "version": "15.x-pg15"
  },
  "redis": {"connected": true},
  "vault": {"connected": true}
}
```

##### 3. **ç›´æ¥DBæ¥ç¶šç¢ºèª**
```bash
# ã‚³ãƒ³ãƒ†ãƒŠå†…ã§psqlæ¥ç¶š
docker-compose exec timescaledb psql -U cqox_user -d cqox_db

# TimescaleDBæ‹¡å¼µç¢ºèª
cqox_db=# SELECT * FROM pg_extension WHERE extname = 'timescaledb';

# Hypertableç¢ºèª
cqox_db=# SELECT * FROM timescaledb_information.hypertables;
```

##### 4. **APIçµŒç”±ã§ã®ç¢ºèª**
```bash
# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç™»éŒ²
curl -X POST http://localhost:8080/api/dataset/upload \
  -F "file=@data.csv" \
  -F "dataset_id=test_001"

# ã‚¸ãƒ§ãƒ–å®Ÿè¡Œ
curl -X POST http://localhost:8080/api/job/create \
  -H "Content-Type: application/json" \
  -d '{"dataset_id": "test_001", "estimator": "did"}'

# çµæœç¢ºèªï¼ˆDBã«ä¿å­˜ã•ã‚Œã‚‹ï¼‰
curl http://localhost:8080/api/job/{job_id}/results
```

#### ã“ã¡ã‚‰å´ï¼ˆé–‹ç™ºè€…å´ï¼‰ã®ç¢ºèªæ–¹æ³•

##### 1. **ã‚³ãƒ¼ãƒ‰å†…ã§ã®DBæ¥ç¶šç¢ºèª**

**å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«**: `backend/db/postgres_client.py`

```python
# PostgreSQLæ¥ç¶š
postgres_client = PostgresClient()

# æ¥ç¶šç¢ºèª
if postgres_client.conn:
    print("âœ… PostgreSQL connected")
else:
    print("âŒ PostgreSQL not connected")
```

##### 2. **TimescaleDBè¨­å®šã®ç¢ºèª**

**å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«**: `backend/db/timescaledb_config.py`

```python
from backend.db.timescaledb_config import TimescaleDBConfig

config = TimescaleDBConfig()

# Hypertableä½œæˆ
config.setup_timescaledb()

# æ¤œè¨¼
config.verify_hypertables()
# Output:
# âœ… jobs hypertable created
# âœ… Compression policy active
# âœ… Retention policy: 90 days
```

##### 3. **ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ç®¡ç†ã®ç¢ºèª**

**å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«**: `backend/db/transaction_manager.py`

```python
from backend.db.transaction_manager import TransactionManager

tx_manager = TransactionManager()

# ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œï¼ˆè‡ªå‹•ãƒªãƒˆãƒ©ã‚¤ï¼‰
with tx_manager.transaction() as session:
    session.execute(text("INSERT INTO jobs ..."))
    # è‡ªå‹•commitã€ã‚¨ãƒ©ãƒ¼æ™‚ã¯è‡ªå‹•rollback + retry
```

##### 4. **ç’°å¢ƒå¤‰æ•°ã§ã®è¨­å®š**

```bash
# .env.production
DATABASE_URL=postgresql://cqox_user:${DB_PASSWORD}@timescaledb:5432/cqox_db
DB_POOL_SIZE=20
DB_MAX_OVERFLOW=10
```

##### 5. **ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã§ã®ç›£è¦–**

```bash
# Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹ç¢ºèª
curl http://localhost:8080/metrics | grep db_

# Output:
# db_connections_active 5
# db_connections_idle 15
# db_query_duration_seconds_bucket{le="0.01"} 125
```

---

### 6. æ¨å®šé‡ã«å½“ã¦ã¯ã¾ã‚‰ãªã„ã‚«ãƒ©ãƒ ã®è¨­è¨ˆ

**å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«**: `backend/inference/estimator_validator.py` (Line 93-260)

#### ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥

##### 1. **æ¨å®šé‡ã®è¦ä»¶å®šç¾©**

```python
ESTIMATOR_SPECS = {
    "did": EstimatorRequirements(
        name="Difference-in-Differences",
        required=["y", "treatment", "unit_id", "time"],  # â† å¿…é ˆ
        optional=["covariates"],                         # â† ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        fallback="tvce"                                  # â† ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å…ˆ
    ),
    "iv": EstimatorRequirements(
        name="Instrumental Variables (2SLS)",
        required=["y", "treatment", "z"],  # z = æ“ä½œå¤‰æ•°
        optional=["unit_id", "covariates"],
        fallback="tvce"
    ),
    # ... å…¨20æ¨å®šé‡å®šç¾©
}
```

##### 2. **ã‚«ãƒ©ãƒ æ¤œè¨¼**

```python
def validate_estimator(self, estimator: str) -> Dict:
    """æ¨å®šé‡ãŒå®Ÿè¡Œå¯èƒ½ã‹æ¤œè¨¼"""
    spec = ESTIMATOR_SPECS[estimator]

    # å¿…é ˆã‚«ãƒ©ãƒ ãƒã‚§ãƒƒã‚¯
    missing_required = []
    for role in spec.required:
        col = self.mapping.get(role)
        if not col or col not in self.available_columns:
            missing_required.append(role)

    can_run = len(missing_required) == 0

    return {
        "can_run": can_run,
        "missing_required": missing_required,
        "fallback": spec.fallback if not can_run else None,
        "message": "âœ“ DID can run" if can_run else
                   "âœ— DID cannot run - missing: time (will use fallback: tvce)"
    }
```

##### 3. **è‡ªå‹•ã‚«ãƒ©ãƒ æ¤œå‡º**

```python
def auto_detect_missing_columns(self) -> Dict[str, str]:
    """æ¬ è½ã‚«ãƒ©ãƒ ã‚’è‡ªå‹•æ¤œå‡º"""
    # æ¨™æº–ã‚«ãƒ©ãƒ æ¤œå‡º
    selector = ColumnSelector(self.df)
    selection = selector.select_columns(confidence_threshold=0.2)

    # ç‰¹æ®Šã‚«ãƒ©ãƒ æ¤œå‡º
    # - log_propensity: "propensity", "prob", "score"
    # - z (instrument): "instrument", "iv", "z"
    # - cluster_id: "cluster", "group", "cohort"
    # - domain: "domain", "site", "location"

    return detected_columns
```

##### 4. **ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒã‚§ãƒ¼ãƒ³**

```
ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šæ¨å®šé‡ï¼ˆä¾‹: IVï¼‰
    â†“ æ¤œè¨¼
å¿…é ˆã‚«ãƒ©ãƒ ä¸è¶³ï¼ˆz ãŒç„¡ã„ï¼‰
    â†“ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
TVCEï¼ˆTime-Varying Causal Effectsï¼‰
    â†“ æ¤œè¨¼
å¿…é ˆã‚«ãƒ©ãƒ ä¸è¶³ï¼ˆtime ãŒç„¡ã„ï¼‰
    â†“ æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
Simple Diffï¼ˆå˜ç´”å·®åˆ†ï¼‰â† ã“ã‚Œã¯å¸¸ã«å®Ÿè¡Œå¯èƒ½
```

##### 5. **å®Ÿè¡Œå¯èƒ½æ¨å®šé‡ã®è‡ªå‹•æç¤º**

```python
def get_runnable_estimators(self) -> List[str]:
    """ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã§å®Ÿè¡Œå¯èƒ½ãªæ¨å®šé‡ã‚’å–å¾—"""
    validation = self.validate_all()
    runnable = [name for name, result in validation.items()
                if result["can_run"]]

    # Outputä¾‹:
    # ["tvce", "simple_diff", "psm", "ipw"]
    # â†’ DID, IV, SCMã¯å®Ÿè¡Œä¸å¯ï¼ˆunit_id, time, zç­‰ãŒç„¡ã„ï¼‰
```

#### ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ã‚¯ã‚¹ãƒšãƒªã‚¨ãƒ³ã‚¹

**APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹**:
```json
{
  "validation": {
    "did": {
      "can_run": false,
      "missing_required": ["time"],
      "fallback": "tvce",
      "message": "âœ— DID cannot run - missing: time (will use fallback: tvce)"
    },
    "tvce": {
      "can_run": true,
      "missing_optional": [],
      "message": "âœ“ TVCE can run"
    }
  },
  "runnable": ["tvce", "simple_diff", "psm", "ipw"],
  "suggestions": {
    "time": "date_column"  // â† è‡ªå‹•æ¤œå‡ºã®ææ¡ˆ
  }
}
```

---

### 7. å¯è¦–åŒ–ã«å½“ã¦ã¯ã¾ã‚‰ãªã„ã‚«ãƒ©ãƒ ã®è¨­è¨ˆ

**å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«**: `backend/engine/figure_selector.py` (Line 19-200)

#### å¯è¦–åŒ–è¦ä»¶å®šç¾©

##### 1. **ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥å›³è¡¨ã®è¦ä»¶**

```python
FIGURE_REQUIREMENTS = {
    "medical_km_survival": {
        "required_columns": ["y", "treatment"],
        "optional_columns": ["time"],
        "min_rows": 50,
        "description": "KM-style survival curves"
    },
    "retail_uplift_curve": {
        "required_columns": ["y", "treatment"],
        "min_rows": 100,
        "description": "Uplift curve for targeting"
    },
    "finance_portfolio": {
        "required_columns": [],
        "optional_columns": ["asset_class", "category", "type"],
        "min_rows": 10,
        "description": "Portfolio allocation split"
    },
    # ... 20+ ãƒ‰ãƒ¡ã‚¤ãƒ³å›³è¡¨
}
```

##### 2. **æŸ”è»Ÿãªè¦ä»¶å®šç¾©**

| è¦ä»¶ã‚¿ã‚¤ãƒ— | èª¬æ˜ | ä¾‹ |
|-----------|------|-----|
| `required_columns` | å¿…é ˆã‚«ãƒ©ãƒ  | ["y", "treatment"] |
| `optional_columns` | ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚«ãƒ©ãƒ  | ["time", "covariates"] |
| `required_one_of` | ã„ãšã‚Œã‹1ã¤å¿…é ˆ | ["cluster_id", "site_id", "hospital_id"] |
| `min_rows` | æœ€å°è¡Œæ•° | 50 |
| `min_dose_levels` | æœ€å°æŠ•ä¸ãƒ¬ãƒ™ãƒ«æ•° | 3 |
| `min_clusters` | æœ€å°ã‚¯ãƒ©ã‚¹ã‚¿æ•° | 3 |
| `min_time_periods` | æœ€å°æ™‚ç³»åˆ—æœŸé–“æ•° | 5 |

##### 3. **å¯è¦–åŒ–é¸æŠãƒ­ã‚¸ãƒƒã‚¯**

```python
class FigureSelector:
    def select_figures(self, df: pd.DataFrame, mapping: Dict) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ç”Ÿæˆå¯èƒ½ãªå›³è¡¨ã‚’é¸æŠ"""
        available_figures = []

        for fig_name, requirements in FIGURE_REQUIREMENTS.items():
            if self._can_generate(df, mapping, requirements):
                available_figures.append(fig_name)

        return {
            "available": available_figures,
            "total": len(FIGURE_REQUIREMENTS),
            "coverage": len(available_figures) / len(FIGURE_REQUIREMENTS)
        }

    def _can_generate(self, df, mapping, req) -> bool:
        """è¦ä»¶ã‚’æº€ãŸã™ã‹ç¢ºèª"""
        # 1. å¿…é ˆã‚«ãƒ©ãƒ ãƒã‚§ãƒƒã‚¯
        for col_role in req.get("required_columns", []):
            if col_role not in mapping:
                return False

        # 2. ã„ãšã‚Œã‹1ã¤å¿…é ˆãƒã‚§ãƒƒã‚¯
        if "required_one_of" in req:
            if not any(role in mapping for role in req["required_one_of"]):
                return False

        # 3. æœ€å°è¡Œæ•°ãƒã‚§ãƒƒã‚¯
        if len(df) < req.get("min_rows", 0):
            return False

        # 4. ãã®ä»–ã®æ¡ä»¶ãƒã‚§ãƒƒã‚¯ï¼ˆdose levels, clustersç­‰ï¼‰
        # ...

        return True
```

##### 4. **ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯è¦–åŒ–**

**å¸¸ã«åˆ©ç”¨å¯èƒ½ãªåŸºæœ¬å›³è¡¨**:
```python
FALLBACK_FIGURES = [
    "ate_density",           # ATEå¯†åº¦ãƒ—ãƒ­ãƒƒãƒˆï¼ˆy, treatmentã®ã¿ï¼‰
    "covariate_balance",     # å…±å¤‰é‡ãƒãƒ©ãƒ³ã‚¹ï¼ˆy, treatmentã®ã¿ï¼‰
    "treatment_distribution", # æ²»ç™‚åˆ†å¸ƒï¼ˆtreatmentã®ã¿ï¼‰
    "outcome_distribution"   # ã‚¢ã‚¦ãƒˆã‚«ãƒ åˆ†å¸ƒï¼ˆyã®ã¿ï¼‰
]
```

##### 5. **æ®µéšçš„å¯è¦–åŒ–**

```
Level 1: æœ€å°é™ï¼ˆy, treatment ã®ã¿ï¼‰
â”œâ”€ ATEå¯†åº¦ãƒ—ãƒ­ãƒƒãƒˆ
â”œâ”€ æ²»ç™‚åŠ¹æœåˆ†å¸ƒ
â””â”€ å…±å¤‰é‡ãƒãƒ©ãƒ³ã‚¹ï¼ˆåˆ©ç”¨å¯èƒ½ãªã‚‰ï¼‰

Level 2: æ™‚ç³»åˆ—è¿½åŠ ï¼ˆ+ timeï¼‰
â”œâ”€ ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ã‚¿ãƒ‡ã‚£
â”œâ”€ ä¸¦è¡Œãƒˆãƒ¬ãƒ³ãƒ‰
â””â”€ æ²»ç™‚åŠ¹æœã®æ™‚ç³»åˆ—æ¨ç§»

Level 3: ãƒ‘ãƒãƒ«è¿½åŠ ï¼ˆ+ unit_id + timeï¼‰
â”œâ”€ Difference-in-Differenceså¯è¦–åŒ–
â”œâ”€ åˆæˆã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«
â””â”€ ãƒ‘ãƒãƒ«ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—

Level 4: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¿½åŠ ï¼ˆ+ cluster_id, neighbor_exposureï¼‰
â”œâ”€ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯3D
â”œâ”€ ã‚¹ãƒ”ãƒ«ã‚ªãƒ¼ãƒãƒ¼åŠ¹æœ
â””â”€ ã‚¯ãƒ©ã‚¹ã‚¿åŠ¹æœ
```

#### ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ã‚¯ã‚¹ãƒšãƒªã‚¨ãƒ³ã‚¹

**APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹**:
```json
{
  "available_figures": [
    "ate_density",
    "covariate_balance",
    "treatment_distribution",
    "retail_uplift_curve"
  ],
  "unavailable_figures": [
    {
      "name": "medical_km_survival",
      "reason": "missing optional: time",
      "can_enable_by": "adding time column"
    },
    {
      "name": "education_event_study",
      "reason": "missing required: time",
      "can_enable_by": "adding time column"
    },
    {
      "name": "network_3d",
      "reason": "missing required: cluster_id, neighbor_exposure",
      "can_enable_by": "adding network columns"
    }
  ],
  "coverage": "4/20 (20%)",
  "recommendations": [
    "Add 'time' column to unlock 8 more visualizations",
    "Add 'cluster_id' to unlock network visualizations"
  ]
}
```

---

### 8. ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã®ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆ

**å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«**: `backend/engine/production_outputs.py` (Line 1-150)

#### ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆä¸€è¦§

##### 1. **ãƒãƒªã‚·ãƒ¼é…ä¿¡ãƒ•ã‚¡ã‚¤ãƒ«** (Policy Distribution Files)

**å½¢å¼**: CSV / Parquet
**ç›®çš„**: æœ¬ç•ªç’°å¢ƒã§ã®ãƒãƒªã‚·ãƒ¼é©ç”¨

```python
# ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«
policy_{dataset_id}_{scenario_id}_{timestamp}.parquet

# å†…å®¹
{
    "unit_id": [1, 2, 3, ...],           # ãƒ¦ãƒ‹ãƒƒãƒˆID
    "treatment": [1, 0, 1, ...],         # æ–°ãƒãƒªã‚·ãƒ¼ï¼ˆ0/1ï¼‰
    "score": [0.85, 0.32, 0.91, ...],   # uplift score
    "rank": [1, 1000, 2, ...],           # ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    "scenario_id": "optimal_profit",
    "generated_at": "2025-11-10T01:30:00Z"
}
```

**ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹**:
- A/Bãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ ã¸ã®é…ä¿¡
- ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã‚ªãƒ¼ãƒˆãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³
- ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ³ã‚¸ãƒ³

##### 2. **å“è³ªã‚²ãƒ¼ãƒˆãƒ¬ãƒãƒ¼ãƒˆ** (Quality Gates Reports)

**å½¢å¼**: JSON / CSV
**ç›®çš„**: ç›£æŸ»ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

```json
{
  "dataset_id": "retail_campaign_2024",
  "scenario_id": "optimal_targeting",
  "decision": "GO",  // GO / CANARY / HOLD
  "pass_rate": 0.95,
  "gates_summary": {
    "PASS": 19,
    "FAIL": 0,
    "WARNING": 1
  },
  "gates_detail": [
    {"name": "parallel_trends", "status": "PASS", "score": 0.98},
    {"name": "covariate_balance", "status": "PASS", "score": 0.92},
    {"name": "overlap", "status": "WARNING", "score": 0.85}
  ]
}
```

**æ±ºå®šãƒ­ã‚¸ãƒƒã‚¯**:
- **GO**: ã™ã¹ã¦ã®ã‚²ãƒ¼ãƒˆPASS â†’ æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤å¯
- **CANARY**: ä¸€éƒ¨WARNING â†’ ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤æ¨å¥¨
- **HOLD**: FAILå­˜åœ¨ â†’ ãƒ‡ãƒ—ãƒ­ã‚¤ä¸å¯

##### 3. **ç›£æŸ»è¨¼è·¡** (Audit Trail)

**å½¢å¼**: JSONL (JSON Lines)
**ç›®çš„**: ã‚¤ãƒŸãƒ¥ãƒ¼ã‚¿ãƒ–ãƒ«ãƒ­ã‚°

```jsonl
{"timestamp": "2025-11-10T01:00:00Z", "event": "scenario_run", "user_id": "alice", "dataset_id": "retail_001", "scenario_id": "optimal", "details": {...}}
{"timestamp": "2025-11-10T01:05:00Z", "event": "quality_gates", "decision": "GO", "pass_rate": 0.95}
{"timestamp": "2025-11-10T01:10:00Z", "event": "deployment", "target": "production", "policy_file": "policy_retail_001_optimal_20251110.parquet"}
```

**ç‰¹å¾´**:
- âœ… è¿½è¨˜å°‚ç”¨ï¼ˆAppend-onlyï¼‰
- âœ… ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ã
- âœ… å®Œå…¨ãªç›£æŸ»è¨¼è·¡

##### 4. **æ´¾ç”Ÿã‚«ãƒ©ãƒ å°å¸³** (Derivation Ledger)

**å½¢å¼**: JSON
**ç›®çš„**: é€æ˜æ€§ã¨å†ç¾æ€§

```json
{
  "dataset_id": "retail_001",
  "derived_columns": [
    {
      "name": "propensity_score",
      "formula": "LogisticRegression(treatment ~ age + region + ...)",
      "created_at": "2025-11-10T01:00:00Z",
      "dependencies": ["age", "region", "purchase_history"]
    },
    {
      "name": "uplift_score",
      "formula": "CATE(y | treatment=1) - CATE(y | treatment=0)",
      "created_at": "2025-11-10T01:02:00Z",
      "dependencies": ["y", "treatment", "covariates"]
    }
  ]
}
```

##### 5. **æ„æ€æ±ºå®šã‚«ãƒ¼ãƒ‰** (Decision Cards)

**å½¢å¼**: PDF / HTML
**ç›®çš„**: ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼

**å†…å®¹**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Decision Card: Optimal Targeting        â”‚
â”‚ Dataset: retail_campaign_2024           â”‚
â”‚ Generated: 2025-11-10 01:30 UTC         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚ RECOMMENDATION: GO âœ…                    â”‚
â”‚                                         â”‚
â”‚ Key Findings:                           â”‚
â”‚ â€¢ ATE: +$2.5 per user (95% CI: 1.2-3.8)â”‚
â”‚ â€¢ ROI: 250% (intervention cost: $1)    â”‚
â”‚ â€¢ Coverage: 80% (targeting top users)  â”‚
â”‚ â€¢ Expected profit: $1,250 (+150%)      â”‚
â”‚                                         â”‚
â”‚ Quality Gates: 19/20 PASS (95%) âœ…      â”‚
â”‚                                         â”‚
â”‚ Risks:                                  â”‚
â”‚ âš ï¸  Overlap ratio: 85% (< 90% target)   â”‚
â”‚ âœ… All other gates passed               â”‚
â”‚                                         â”‚
â”‚ Action Items:                           â”‚
â”‚ 1. Deploy to production                â”‚
â”‚ 2. Monitor for 7 days                  â”‚
â”‚ 3. Re-evaluate monthly                 â”‚
â”‚                                         â”‚
â”‚ Approved by: ____________  Date: _____ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

##### 6. **WolframONEå¯è¦–åŒ–** (Visualizations)

**å½¢å¼**: HTML (ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–)
**ç›®çš„**: æŠ€è¡“è€…ãƒ»æ„æ€æ±ºå®šè€…å‘ã‘å¯è¦–åŒ–

**ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«**:
```
reports/figures/
â”œâ”€â”€ ate_density__S0.html
â”œâ”€â”€ ate_density__S1_optimal.html
â”œâ”€â”€ network_3d__S0.html
â”œâ”€â”€ network_3d__S1_optimal.html
â”œâ”€â”€ policy_frontier.html
â””â”€â”€ ...
```

##### 7. **è‡ªå‹•ç”ŸæˆãƒŠãƒ©ãƒ†ã‚£ãƒ–** (Narrative)

**å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«**: `backend/reporting/narrative_generator.py`

**å½¢å¼**: Markdown / JSON
**å†…å®¹**:
```markdown
# Executive Summary (TL;DR)

The intervention increased revenue by $2.5 per user (95% CI: $1.2-$3.8).
This effect is statistically significant (p < 0.001) and economically meaningful.

## Key Insights

- **ROI**: 250% (based on $1 intervention cost)
- **Spillover**: +15% from network effects
- **Risk**: Low (narrow confidence interval)

## Strategic Recommendation

1. Scale intervention to full population
2. Monitor spillover effects
3. Re-evaluate in 3 months

## Financial Impact

- Current profit: $500
- Expected profit (optimal policy): $1,250
- Incremental profit: +$750 (+150%)
```

---

## ğŸ“Š ç·åˆã¾ã¨ã‚

### ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ•ãƒ­ãƒ¼å…¨ä½“

```
1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
   â”œâ”€ CSV/TSV/JSON/Parquet/Excel/Featherå¯¾å¿œ
   â”œâ”€ Magic number validation
   â””â”€ UTF-8å®Œå…¨å¯¾å¿œï¼ˆæ—¥æœ¬èªOKï¼‰

2. ã‚«ãƒ©ãƒ è‡ªå‹•æ¤œå‡ºï¼ˆç²¾åº¦85%ï¼‰
   â”œâ”€ Outcome (y)
   â”œâ”€ Treatment
   â”œâ”€ Unit ID
   â””â”€ Time

3. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
   â”œâ”€ æ¬ æå€¤è£œå®Œ
   â”œâ”€ æ¨™æº–åŒ–
   â”œâ”€ Propensity Scoreè¨ˆç®—
   â””â”€ SMDè¨ˆç®—

4. å“è³ªã‚²ãƒ¼ãƒˆ
   â”œâ”€ Overlap ratio â‰¥ 0.1
   â””â”€ Max |SMD| â‰¤ 0.1

5. Parquetä¿å­˜ï¼ˆæœ€çµ‚å½¢å¼ï¼‰
   â””â”€ data.parquet + metadata.json

6. æ¨å®šé‡é¸æŠ
   â”œâ”€ 20æ¨å®šé‡ã‹ã‚‰è‡ªå‹•é¸æŠ
   â”œâ”€ å¿…é ˆã‚«ãƒ©ãƒ ãƒã‚§ãƒƒã‚¯
   â””â”€ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½

7. å¯è¦–åŒ–é¸æŠ
   â”œâ”€ 42+å›³è¡¨ã‹ã‚‰è‡ªå‹•é¸æŠ
   â”œâ”€ ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãå¯ç”¨æ€§åˆ¤å®š
   â””â”€ æ®µéšçš„å¯è¦–åŒ–

8. ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆç”Ÿæˆ
   â”œâ”€ ãƒãƒªã‚·ãƒ¼é…ä¿¡ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆParquetï¼‰
   â”œâ”€ å“è³ªã‚²ãƒ¼ãƒˆãƒ¬ãƒãƒ¼ãƒˆï¼ˆJSONï¼‰
   â”œâ”€ ç›£æŸ»è¨¼è·¡ï¼ˆJSONLï¼‰
   â”œâ”€ æ„æ€æ±ºå®šã‚«ãƒ¼ãƒ‰ï¼ˆPDF/HTMLï¼‰
   â”œâ”€ WolframONEå¯è¦–åŒ–ï¼ˆHTMLï¼‰
   â””â”€ è‡ªå‹•ãƒŠãƒ©ãƒ†ã‚£ãƒ–ï¼ˆMarkdownï¼‰
```

---

**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: âœ… **å®Œå…¨èª¿æŸ»å®Œäº†**
**å®Ÿè£…çŠ¶æ³**: ã™ã¹ã¦æœ¬ç•ªç’°å¢ƒå¯¾å¿œæ¸ˆã¿
**æœ€çµ‚æ›´æ–°**: 2025-11-10
