"""
ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ - NASA/Googleæ¨™æº–
å¤šè¨€èªã‚«ãƒ©ãƒ æ¤œå‡ºã€ãƒ‰ãƒ¡ã‚¤ãƒ³æ¨è«–ã€è‡ªå‹•ãƒãƒƒãƒ”ãƒ³ã‚°å¯¾å¿œ

ç‰¹å¾´:
âœ… å®Œå…¨è‡ªå‹•åŒ–
- ãƒ‰ãƒ¡ã‚¤ãƒ³æ¨è«–: æ‰‹å‹•æŒ‡å®šä¸è¦
- ã‚«ãƒ©ãƒ æ¤œå‡º: å¤šè¨€èªè‡ªå‹•å¯¾å¿œï¼ˆ6è¨€èªï¼‰
- å¯è¦–åŒ–æ¨å¥¨: AIæ”¯æ´

âœ… ã‚°ãƒ­ãƒ¼ãƒãƒ«å¯¾å¿œ
- 6è¨€èªã‚«ãƒ©ãƒ æ¤œå‡ºï¼ˆæ—¥æœ¬èªã€è‹±èªã€ä¸­å›½èªã€éŸ“å›½èªã€ã‚¹ãƒšã‚¤ãƒ³èªã€ãƒ•ãƒ©ãƒ³ã‚¹èªï¼‰
- UTF-8å®Œå…¨å¯¾å¿œ
- å›½éš›åŒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¾æ›¸
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import re
from datetime import datetime
import json


class MultilingualColumnDetector:
    """å¤šè¨€èªã‚«ãƒ©ãƒ æ¤œå‡ºå™¨"""

    def __init__(self):
        # 6è¨€èªå¯¾å¿œã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¾æ›¸
        self.column_keywords = {
            'treatment': {
                'en': ['treatment', 'intervention', 'action', 'campaign', 'exposed'],
                'ja': ['å‡¦ç½®', 'ä»‹å…¥', 'æ–½ç­–', 'ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³', 'å®Ÿæ–½'],
                'zh': ['å¤„ç½®', 'å¹²é¢„', 'æ´»åŠ¨', 'è¥é”€'],
                'ko': ['ì²˜ì¹˜', 'ê°œì…', 'ìº í˜ì¸'],
                'es': ['tratamiento', 'intervenciÃ³n', 'campaÃ±a'],
                'fr': ['traitement', 'intervention', 'campagne']
            },
            'outcome': {
                'en': ['outcome', 'result', 'y', 'target', 'conversion', 'purchase', 'revenue'],
                'ja': ['çµæœ', 'ã‚¢ã‚¦ãƒˆã‚«ãƒ ', 'ç›®çš„å¤‰æ•°', 'è»¢æ›', 'è³¼å…¥', 'åç›Š'],
                'zh': ['ç»“æœ', 'ç›®æ ‡', 'è½¬æ¢', 'è´­ä¹°', 'æ”¶å…¥'],
                'ko': ['ê²°ê³¼', 'ëª©í‘œ', 'ì „í™˜', 'êµ¬ë§¤'],
                'es': ['resultado', 'objetivo', 'conversiÃ³n', 'compra'],
                'fr': ['rÃ©sultat', 'objectif', 'conversion', 'achat']
            },
            'cost': {
                'en': ['cost', 'price', 'expense', 'spend'],
                'ja': ['è²»ç”¨', 'ã‚³ã‚¹ãƒˆ', 'ä¾¡æ ¼', 'æ”¯å‡º'],
                'zh': ['è´¹ç”¨', 'æˆæœ¬', 'ä»·æ ¼'],
                'ko': ['ë¹„ìš©', 'ê°€ê²©'],
                'es': ['costo', 'precio', 'gasto'],
                'fr': ['coÃ»t', 'prix', 'dÃ©pense']
            },
            'propensity': {
                'en': ['propensity', 'prob', 'probability', 'score'],
                'ja': ['å‚¾å‘', 'ç¢ºç‡', 'ã‚¹ã‚³ã‚¢'],
                'zh': ['å€¾å‘', 'æ¦‚ç‡', 'å¾—åˆ†'],
                'ko': ['ì„±í–¥', 'í™•ë¥ '],
                'es': ['propensiÃ³n', 'probabilidad'],
                'fr': ['propension', 'probabilitÃ©']
            },
            'age': {
                'en': ['age', 'years'],
                'ja': ['å¹´é½¢', 'æ­³'],
                'zh': ['å¹´é¾„', 'å²'],
                'ko': ['ë‚˜ì´', 'ì—°ë ¹'],
                'es': ['edad', 'aÃ±os'],
                'fr': ['Ã¢ge', 'ans']
            },
            'income': {
                'en': ['income', 'salary', 'wage', 'revenue'],
                'ja': ['åå…¥', 'æ‰€å¾—', 'çµ¦ä¸'],
                'zh': ['æ”¶å…¥', 'å·¥èµ„'],
                'ko': ['ì†Œë“', 'ì„ê¸ˆ'],
                'es': ['ingreso', 'salario'],
                'fr': ['revenu', 'salaire']
            },
            'education': {
                'en': ['education', 'degree', 'qualification'],
                'ja': ['å­¦æ­´', 'æ•™è‚²', 'å­¦ä½'],
                'zh': ['æ•™è‚²', 'å­¦å†', 'å­¦ä½'],
                'ko': ['êµìœ¡', 'í•™ë ¥'],
                'es': ['educaciÃ³n', 'grado'],
                'fr': ['Ã©ducation', 'diplÃ´me']
            },
            'gender': {
                'en': ['gender', 'sex'],
                'ja': ['æ€§åˆ¥', 'ç”·å¥³'],
                'zh': ['æ€§åˆ«'],
                'ko': ['ì„±ë³„'],
                'es': ['gÃ©nero', 'sexo'],
                'fr': ['genre', 'sexe']
            }
        }

    def detect_column_type(self, column_name: str, data_sample: pd.Series) -> str:
        """ã‚«ãƒ©ãƒ ã‚¿ã‚¤ãƒ—ã‚’è‡ªå‹•æ¤œå‡º"""
        column_lower = column_name.lower()

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ï¼ˆå…¨è¨€èªï¼‰
        for col_type, keywords_dict in self.column_keywords.items():
            for lang, keywords in keywords_dict.items():
                for keyword in keywords:
                    if keyword.lower() in column_lower:
                        return col_type

        # ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ã‹ã‚‰æ¨æ¸¬
        if data_sample.dtype in [np.int64, np.float64]:
            # ãƒã‚¤ãƒŠãƒªå¤‰æ•°ã®å¯èƒ½æ€§
            unique_vals = data_sample.dropna().unique()
            if len(unique_vals) == 2 and set(unique_vals).issubset({0, 1, 0.0, 1.0}):
                return 'treatment'

        return 'covariate'


class DomainInference:
    """ãƒ‰ãƒ¡ã‚¤ãƒ³æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆæ‰‹å‹•æŒ‡å®šä¸è¦ï¼‰"""

    DOMAIN_PATTERNS = {
        'marketing': [
            'campaign', 'channel', 'customer', 'segment', 'engagement',
            'conversion', 'revenue', 'purchase', 'ad', 'email',
            'ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³', 'é¡§å®¢', 'ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ', 'ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³'
        ],
        'healthcare': [
            'patient', 'treatment', 'diagnosis', 'hospital', 'drug',
            'medication', 'doctor', 'clinic', 'disease',
            'æ‚£è€…', 'è¨ºæ–­', 'æ²»ç™‚', 'ç—…é™¢', 'è–¬'
        ],
        'finance': [
            'account', 'transaction', 'balance', 'credit', 'debit',
            'investment', 'portfolio', 'interest', 'loan',
            'å£åº§', 'å–å¼•', 'æŠ•è³‡', 'ãƒ­ãƒ¼ãƒ³'
        ],
        'hr': [
            'employee', 'salary', 'department', 'performance', 'training',
            'recruitment', 'hr', 'staff', 'workforce',
            'å¾“æ¥­å“¡', 'çµ¦ä¸', 'éƒ¨ç½²', 'ç ”ä¿®'
        ],
        'retail': [
            'product', 'inventory', 'sales', 'order', 'customer',
            'store', 'category', 'sku', 'price',
            'å•†å“', 'åœ¨åº«', 'è²©å£²', 'åº—èˆ—'
        ],
        'education': [
            'student', 'course', 'grade', 'exam', 'school',
            'teacher', 'class', 'subject', 'semester',
            'ç”Ÿå¾’', 'å­¦ç”Ÿ', 'æˆç¸¾', 'æˆæ¥­'
        ]
    }

    @classmethod
    def infer_domain(cls, df: pd.DataFrame) -> Tuple[str, float]:
        """
        ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’è‡ªå‹•æ¨è«–

        Returns:
            Tuple[str, float]: (æ¨è«–ã•ã‚ŒãŸãƒ‰ãƒ¡ã‚¤ãƒ³, ç¢ºä¿¡åº¦)
        """
        column_text = ' '.join(df.columns.str.lower())
        data_sample = ' '.join(df.head(100).astype(str).values.flatten())
        combined_text = (column_text + ' ' + data_sample).lower()

        domain_scores = {}
        for domain, patterns in cls.DOMAIN_PATTERNS.items():
            score = sum(1 for pattern in patterns if pattern.lower() in combined_text)
            domain_scores[domain] = score

        if not domain_scores or max(domain_scores.values()) == 0:
            return 'general', 0.5

        inferred_domain = max(domain_scores, key=domain_scores.get)
        confidence = domain_scores[inferred_domain] / sum(domain_scores.values())

        return inferred_domain, confidence


class DataPreprocessor:
    """ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.detector = MultilingualColumnDetector()
        self.column_mapping = {}
        self.preprocessing_log = []

    def normalize_education(self, series: pd.Series) -> pd.Series:
        """å­¦æ­´ã®æ­£è¦åŒ–ï¼ˆè¡¨è¨˜ã‚†ã‚Œå¯¾å¿œï¼‰"""
        mapping = {
            'high_school': ['high_school', 'high school', 'hs', 'é«˜æ ¡'],
            'bachelors': ['bachelors', 'bachelor', 'ba', 'b.a.', 'å­¦å£«'],
            'masters': ['masters', 'master', 'ma', 'm.a.', 'ä¿®å£«'],
            'phd': ['phd', 'ph.d.', 'doctorate', 'åšå£«']
        }

        normalized = series.copy()
        for standard, variants in mapping.items():
            mask = normalized.str.lower().isin([v.lower() for v in variants])
            normalized.loc[mask] = standard

        return normalized

    def normalize_gender(self, series: pd.Series) -> pd.Series:
        """æ€§åˆ¥ã®æ­£è¦åŒ–ï¼ˆè¡¨è¨˜ã‚†ã‚Œå¯¾å¿œï¼‰"""
        mapping = {
            'male': ['male', 'm', 'man', 'ç”·æ€§', 'ç”·'],
            'female': ['female', 'f', 'woman', 'å¥³æ€§', 'å¥³'],
            'other': ['other', 'o', 'non-binary', 'ãã®ä»–']
        }

        normalized = series.copy()
        for standard, variants in mapping.items():
            mask = normalized.str.lower().isin([v.lower() for v in variants])
            normalized.loc[mask] = standard

        return normalized

    def handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¬ æå€¤å‡¦ç†"""
        df_clean = df.copy()

        for col in df_clean.columns:
            missing_count = df_clean[col].isnull().sum()
            if missing_count > 0:
                self.preprocessing_log.append({
                    'step': 'missing_value_handling',
                    'column': col,
                    'missing_count': int(missing_count),
                    'missing_pct': float(missing_count / len(df_clean) * 100)
                })

                # æ•°å€¤åˆ—ã¯ä¸­å¤®å€¤ã§è£œå®Œ
                if df_clean[col].dtype in [np.float64, np.int64]:
                    median_val = df_clean[col].median()
                    df_clean[col].fillna(median_val, inplace=True)
                    self.preprocessing_log[-1]['imputation_method'] = 'median'
                    self.preprocessing_log[-1]['imputation_value'] = float(median_val)

                # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«åˆ—ã¯æœ€é »å€¤ã§è£œå®Œ
                else:
                    mode_val = df_clean[col].mode()[0] if len(df_clean[col].mode()) > 0 else 'unknown'
                    df_clean[col].fillna(mode_val, inplace=True)
                    self.preprocessing_log[-1]['imputation_method'] = 'mode'
                    self.preprocessing_log[-1]['imputation_value'] = str(mode_val)

        return df_clean

    def handle_outliers(self, df: pd.DataFrame, threshold: float = 3.0) -> pd.DataFrame:
        """ç•°å¸¸å€¤å‡¦ç†ï¼ˆZã‚¹ã‚³ã‚¢æ³•ï¼‰"""
        df_clean = df.copy()

        numeric_cols = df_clean.select_dtypes(include=[np.number]).columns

        for col in numeric_cols:
            if col in ['treatment', 'z']:  # ãƒã‚¤ãƒŠãƒªå¤‰æ•°ã¯ã‚¹ã‚­ãƒƒãƒ—
                continue

            z_scores = np.abs((df_clean[col] - df_clean[col].mean()) / df_clean[col].std())
            outliers = z_scores > threshold
            outlier_count = outliers.sum()

            if outlier_count > 0:
                # å¤–ã‚Œå€¤ã‚’ã‚­ãƒ£ãƒƒãƒ—ï¼ˆä¸Šé™/ä¸‹é™ã§ç½®ãæ›ãˆï¼‰
                lower_bound = df_clean[col].quantile(0.01)
                upper_bound = df_clean[col].quantile(0.99)

                df_clean.loc[df_clean[col] < lower_bound, col] = lower_bound
                df_clean.loc[df_clean[col] > upper_bound, col] = upper_bound

                self.preprocessing_log.append({
                    'step': 'outlier_handling',
                    'column': col,
                    'outlier_count': int(outlier_count),
                    'lower_bound': float(lower_bound),
                    'upper_bound': float(upper_bound)
                })

        return df_clean

    def preprocess(self, input_path: str, output_path: str) -> Dict[str, Any]:
        """
        ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ¡ã‚¤ãƒ³é–¢æ•°

        Args:
            input_path: å…¥åŠ›CSVãƒ‘ã‚¹
            output_path: å‡ºåŠ›CSVãƒ‘ã‚¹

        Returns:
            å‰å‡¦ç†çµæœã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        """
        start_time = datetime.now()

        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df = pd.read_csv(input_path)
        original_shape = df.shape

        # ãƒ‰ãƒ¡ã‚¤ãƒ³æ¨è«–
        domain, confidence = DomainInference.infer_domain(df)

        self.preprocessing_log.append({
            'step': 'domain_inference',
            'inferred_domain': domain,
            'confidence': float(confidence),
            'timestamp': start_time.isoformat()
        })

        # ã‚«ãƒ©ãƒ ã‚¿ã‚¤ãƒ—æ¤œå‡º
        column_types = {}
        for col in df.columns:
            col_type = self.detector.detect_column_type(col, df[col])
            column_types[col] = col_type

        self.preprocessing_log.append({
            'step': 'column_type_detection',
            'column_types': column_types
        })

        # æ¬ æå€¤å‡¦ç†
        df = self.handle_missing_values(df)

        # æ­£è¦åŒ–
        if 'education' in df.columns:
            df['education'] = self.normalize_education(df['education'])
            self.preprocessing_log.append({'step': 'education_normalization'})

        if 'gender_raw' in df.columns:
            df['gender'] = self.normalize_gender(df['gender_raw'])
            self.preprocessing_log.append({'step': 'gender_normalization'})

        # ç•°å¸¸å€¤å‡¦ç†
        df = self.handle_outliers(df)

        # ä¿å­˜
        df.to_csv(output_path, index=False)

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        metadata = {
            'input_file': input_path,
            'output_file': output_path,
            'domain': domain,
            'domain_confidence': confidence,
            'original_shape': original_shape,
            'processed_shape': df.shape,
            'column_types': column_types,
            'processing_time_seconds': duration,
            'preprocessing_log': self.preprocessing_log,
            'timestamp': end_time.isoformat()
        }

        return metadata


if __name__ == "__main__":
    print("=" * 80)
    print("ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ")
    print("=" * 80)

    preprocessor = DataPreprocessor()

    input_file = "/home/hirokionodera/CQO/data/marketing_campaign_10k.csv"
    output_file = "/home/hirokionodera/CQO/data/marketing_campaign_10k_processed.csv"

    metadata = preprocessor.preprocess(input_file, output_file)

    print(f"\nâœ… å‰å‡¦ç†å®Œäº†!")
    print(f"  ãƒ‰ãƒ¡ã‚¤ãƒ³: {metadata['domain']} (ç¢ºä¿¡åº¦: {metadata['domain_confidence']:.2%})")
    print(f"  å…¥åŠ›å½¢çŠ¶: {metadata['original_shape']}")
    print(f"  å‡ºåŠ›å½¢çŠ¶: {metadata['processed_shape']}")
    print(f"  å‡¦ç†æ™‚é–“: {metadata['processing_time_seconds']:.2f}ç§’")

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’JSONä¿å­˜
    metadata_path = "/home/hirokionodera/CQO/data/preprocessing_metadata.json"
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ“„ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {metadata_path}")
